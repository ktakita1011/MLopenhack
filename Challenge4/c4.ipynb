{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "#from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "files = glob.glob('./converted/*/*.jpeg')\n",
    "\n",
    "images = []\n",
    "targets = []\n",
    "\n",
    "for f in files:\n",
    "    # file data to array\n",
    "    arr = np.array(Image.open(f))#.flatten()\n",
    "    images.append(arr)\n",
    "    \n",
    "    target = os.path.basename(os.path.dirname(f))\n",
    "    targets.append(target)\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(images, targets, test_size=0.3)\n",
    "\n",
    "\n",
    "# In[33]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\p000524252\\\\jupyter\\\\Challenge4'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array(data_train)\n",
    "data_test = np.array(data_test)\n",
    "#label_train = np.array(label_train)\n",
    "\n",
    "label = list(set(label_train))\n",
    "label\n",
    "label_train2 = []\n",
    "\n",
    "for i in label_train:\n",
    "    label_train2.append(label.index(i))\n",
    "\n",
    "label_train3 = np_utils.to_categorical(label_train2 , 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[34]:\n",
    "data_train = data_train.astype('float32')/255\n",
    "data_test = data_test.astype('float32')/255\n",
    "\n",
    "classes = 12\n",
    "\n",
    "# data_train = np_utils.to_categorical(data_train, classes)\n",
    "# data_test = np_utils.to_categorical(data_test, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]]\n",
      "\n",
      "  [[0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]]\n",
      "\n",
      "  [[0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.98039216 1.         0.9843137 ]\n",
      "   [0.99607843 0.99607843 0.9882353 ]\n",
      "   [1.         0.98039216 0.99215686]]\n",
      "\n",
      "  [[0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   [0.99607843 0.9882353  1.        ]\n",
      "   [1.         0.9843137  1.        ]]\n",
      "\n",
      "  [[0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [1.         0.98039216 1.        ]\n",
      "   [1.         0.98039216 1.        ]\n",
      "   [1.         0.9882353  1.        ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[0.99607843 0.99607843 0.99607843]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 125, 125, 32)      1568      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 60, 60, 128)       36992     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 60, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1382412   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 1,420,972\n",
      "Trainable params: 1,420,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# In[45]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,4,input_shape=(128,128,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128,3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam(lr=1e-4)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1089 samples, validate on 121 samples\n",
      "Epoch 1/20\n",
      "1089/1089 [==============================] - 20s 19ms/step - loss: 2.1708 - acc: 0.2938 - val_loss: 1.9231 - val_acc: 0.5702\n",
      "Epoch 2/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 1.6299 - acc: 0.5179 - val_loss: 1.4078 - val_acc: 0.5702\n",
      "Epoch 3/20\n",
      "1089/1089 [==============================] - 20s 19ms/step - loss: 1.1707 - acc: 0.6804 - val_loss: 1.0010 - val_acc: 0.7273\n",
      "Epoch 4/20\n",
      "1089/1089 [==============================] - 20s 19ms/step - loss: 0.8086 - acc: 0.7723 - val_loss: 0.6418 - val_acc: 0.8099\n",
      "Epoch 5/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.6247 - acc: 0.8044 - val_loss: 0.5468 - val_acc: 0.8512\n",
      "Epoch 6/20\n",
      "1089/1089 [==============================] - 21s 19ms/step - loss: 0.4904 - acc: 0.8558 - val_loss: 0.4662 - val_acc: 0.8512\n",
      "Epoch 7/20\n",
      "1089/1089 [==============================] - 21s 19ms/step - loss: 0.3834 - acc: 0.8916 - val_loss: 0.4999 - val_acc: 0.8182\n",
      "Epoch 8/20\n",
      "1089/1089 [==============================] - 20s 19ms/step - loss: 0.3441 - acc: 0.8972 - val_loss: 0.4886 - val_acc: 0.8182\n",
      "Epoch 9/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.3062 - acc: 0.9017 - val_loss: 0.4476 - val_acc: 0.8264\n",
      "Epoch 10/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.2528 - acc: 0.9293 - val_loss: 0.3484 - val_acc: 0.8512\n",
      "Epoch 11/20\n",
      "1089/1089 [==============================] - 20s 19ms/step - loss: 0.2217 - acc: 0.9275 - val_loss: 0.3179 - val_acc: 0.8926\n",
      "Epoch 12/20\n",
      "1089/1089 [==============================] - 20s 19ms/step - loss: 0.2013 - acc: 0.9403 - val_loss: 0.3512 - val_acc: 0.8843\n",
      "Epoch 13/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1705 - acc: 0.9513 - val_loss: 0.2979 - val_acc: 0.9091\n",
      "Epoch 14/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1542 - acc: 0.9550 - val_loss: 0.3039 - val_acc: 0.9091\n",
      "Epoch 15/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1427 - acc: 0.9550 - val_loss: 0.2932 - val_acc: 0.9091\n",
      "Epoch 16/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1330 - acc: 0.9605 - val_loss: 0.3048 - val_acc: 0.8926\n",
      "Epoch 17/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1285 - acc: 0.9624 - val_loss: 0.3297 - val_acc: 0.8760\n",
      "Epoch 18/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1219 - acc: 0.9642 - val_loss: 0.2705 - val_acc: 0.9008\n",
      "Epoch 19/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.1057 - acc: 0.9679 - val_loss: 0.2831 - val_acc: 0.9174\n",
      "Epoch 20/20\n",
      "1089/1089 [==============================] - 20s 18ms/step - loss: 0.0991 - acc: 0.9752 - val_loss: 0.2795 - val_acc: 0.9091\n"
     ]
    }
   ],
   "source": [
    "# In[51]:\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "nb_epoch = 20\n",
    "history = model.fit(data_train, label_train3, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_split=0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "label = list(set(label_test))\n",
    "label\n",
    "label_test2 = []\n",
    "\n",
    "for i in label_test:\n",
    "    label_test2.append(label.index(i))\n",
    "\n",
    "label_test3 = np_utils.to_categorical(label_test2 , 12)\n",
    "print(label_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.2163160737596725\n",
      "acc 0.8439306358381503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, acc = model.evaluate(data_test, label_test3, verbose=0)\n",
    "print(\"loss\",loss)\n",
    "print(\"acc\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[41]:\n",
    "data_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[56]:\n",
    "history.history\n",
    "# In[6]:\n",
    "print(model.summary())\n",
    "# In[7]:\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
